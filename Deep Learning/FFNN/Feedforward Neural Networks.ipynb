{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward process\n",
    "<img src=\"images/feedforward_1.png\" alt=\"drawing\" width=\"400px\"/>\n",
    "<img src=\"images/feedforward_2.png\" alt=\"drawing\" width=\"400px\"/>\n",
    "\n",
    "Vector h' of the hidden layer will be calculated by multiplying the input vector with the weight matrix $W^1$ the following way:\n",
    "\n",
    "$\\overline{h'}=(\\bar xW^1)$ \n",
    "\n",
    "After finding h', we need an **activation function** ($\\Phi$) to finalize the computation of the hidden layer's values. This activation function can be a $Hyperbolic Tangent$, a $Sigmoid$or a $ReLU$ function. We can use the following two equations to express the final hidden vector $\\bar{h}$ :\n",
    "\n",
    "$\\overline{h}=\\Phi(\\bar xW^1)$  \n",
    "\n",
    "or \n",
    "\n",
    "$\\overline{h}=\\Phi(\\overline{h'})$  \n",
    "\n",
    "$Activation Function$:\n",
    "- To make sure that the values of h do not explode or increase too much in size.\n",
    "- Allows the network to represent nonlinear relationships between its inputs and its outputs.\n",
    "- Most real world data is nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation theory\n",
    "- In the backpropagation process we minimize the network error slightly with each iteration, by adjusting the weights.\n",
    "- Weight Update:\n",
    "\n",
    "  $W_{new} = W_{previous} + \\large\\alpha\\left(-\\frac{\\partial E}{\\partial W}\\right)$\n",
    "  \n",
    "  $\\large\\alpha :$ Learning rate\n",
    "  \n",
    "  $\\large\\frac{\\partial E}{\\partial W} :$ Partial Derivate. Let us measure how the error is impacted by each weights seperately.\n",
    "\n",
    "\n",
    "- Since many weights determine the network's output, we can use a vector of the partial derivatives (defined by the Greek \n",
    "  letter Nabla $\\nabla$) of the network error - each with respect to a different weight.\n",
    "  \n",
    "  $W_{new} = W_{previous} + \\large\\alpha\\nabla w \\left(-E\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Training Using Gradient Descent\n",
    "- Updating the weights once every N steps.\n",
    "- $\\large\\delta=\\frac{1}{N}\\sum_{k}^{N}{\\delta_{ijk}}$\n",
    "- Reduction of the complexity of the training process.\n",
    "- Noise reduction (Because of doing average)\n",
    "- Converging faster and more accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
